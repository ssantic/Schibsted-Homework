{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schibsted NLP Machine Learning Challenge #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you’ll be doing text classification on an open data set from Reuters. The task\n",
    "formulation is a bit vague on purpose, enabling you to make your own choices where needed.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the [Reuters data set](www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz)\n",
    "* Your task is to classify the documents to the categories found in TOPICS\n",
    "* Adhere to the train/test split specified in the XML as LEWISSPLIT\n",
    "* If you have time, evaluate and report the results\n",
    "* Please submit a running piece of code, documented as you please (in the code, separate docs, document by tests, whatever makes the code understandable)\n",
    "* We are interested in seeing your ideas, and your coding style/skills. Don’t worry if the classification results are sub-optimal\n",
    "* If you have ideas on how to improve the solution further, please write that down as well\n",
    "* Spend approximately 3 hours on the task\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reuters dataset is given as 22 files in SGM format. SGM is an acronym for Standard Generalized Markup. Files that are saved with the . sgm file extension are written in the SGM programming language. The SGM language is a document generation language that is used to share information digitally using custom tags and DTD file references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each SGM file contains the body of an article, together with other important information, such as the topic(s) of the article, as well as whether it belongs to the training or testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While researching how to parse the SGM files (as they couldn't be parsed using Python's internal `lxml` library), we ran across a [project](https://github.com/ankailou/reuters-preprocessing) on GitHub, written by Ankai Lou, which not only parses the SGM files, but also does a number of preprocessing and feature engineering steps (described below), which prepare our data for modeling. While inspecting the code in detail, we realized that using this library would greatle simplify the approach and solution of the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is our strong believe that Data Scientists should not reinvent the wheel, but build upon the previous work of colleagues, in the interest of efficiency and productivity. After all, one will, for example, rarely code a cross validation scheme from scratch, but rather rely on a pre-existing implementation, such as `scikit-learn`'s `cross_validate` or `KFold` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One small disadvantage of the existing `reuters-preprocessing` package is the fact that it is written for Python 2. While it is possible to have multiple versions of Python on the same machine, this may not be practicle. Thus, in the interest of full reproducibility, we forked the project and amended the code to work in Python 3. We also changed some of the functionality, namely the way the topics are generated, as well as the way TF-IDF values are calculated (instead of the original author's implementation, we opted to use `scikit-learn`'s `TfdfVectorizer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forked and updated version can be found [here](https://github.com/ssantic/reuters-preprocessing), or directly cloned through this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ssantic/reuters-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reuters-preprocessing` library does a number of things:\n",
    "\n",
    "* For each file, the `BeautifulSoup4` library is used to generate a parse-tree from the SGML using the built-in Python `html.parser` library:\n",
    "    * For each parse-tree, article blocks - delimited by `<reuters>` - are separated into strings.\n",
    "    * For each article, the text in `<topics>` and `<places>` delimited by `<d>` is used as class labels; the text in `<title>` and `<body>` is extracted for tokenization.\n",
    "\n",
    "* For each title/body text block, the `NLTK` and `string` libraries are used for tokenization:     \n",
    "    * For each field, digits & unicode symbols are replaced by `None` using the `string` library.\n",
    "    * For each field, punctuation symbols are replaced by `None` using the `string` library.\n",
    "    * For each field, text blocks are tokenized into lists using `nltk.word_tokenize()`.\n",
    "    * For the tokens, `stopwords` from `nltk.corpus` are filtered from the lists.\n",
    "    * For the tokens, non-English words are filtered via `nltk.corpus.wordnet.synsets()`.\n",
    "    * For the tokens, lemmatization is used via `nltk.stem.wordnet.WordLemmatizer()`.\n",
    "    * For the tokens, tokens are stemmed via `nltk.stem.porter.PorterStemmer()`.\n",
    "    * For each stem list, the word stems shorter than 4 characters are filtered from the list.\n",
    "\n",
    "The final output of the text extraction & processing phase is a list of documents:\n",
    "\n",
    "```\n",
    "documents = [document = {'topics' : [], 'places' : [], 'words' : {'title' : [], 'body' : []}}]\n",
    "```\n",
    "\n",
    "From this list, a lexicon is generated for all unique words in titles and body fields:\n",
    "\n",
    "```\n",
    "lexicon = {'title' = set() : 'body' = set()}\n",
    "```\n",
    "\n",
    "This concludes the text extraction and processing phase and prepares the file input for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationale ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several portions of the text processing & tokenization phase are selected for specific reasons:\n",
    "\n",
    "* Digits, unicode characters, and punctuation symbols are removed from the text because digits & meta-characters provide less valuable information to article context than actual words.\n",
    "* Stopwords are removed from the text because words such as `the` are frequently present yet provide no contextual value. Though the TF-IDF process would inevitably filter stopwords during the weighting phase, removing stopwords at tokenization removes several polynomial time calculations during the TF-IDF calculations in linear time - a desirable improvement in performance.\n",
    "* Non-English words are filtered from the text because the stemmer used is the English Porter stemmer; therefore, stemming non-English words is likely to produce erroneous data and artificially inflate the size of the lexicon - which will increase the runtime.\n",
    "* Tokens are lemmatized to reduce the dataset by removing tenses before stemming.\n",
    "* Tokens are stemmed to reduce the dataset further by minimizing the size of the lexicon.\n",
    "* Stems shorter than 4 characters are filtered because sufficiently short stems appear frequently in articles yet provide little importance to classification - similar to stop words.\n",
    "\n",
    "This minification of the tokens & lexicon ensure a minimum number of calculations during the selection phase, while not losing valuable information or context. The same feature reduction process was used for all feature vectors because filtering out low-value words & stems from the data is low-risk/high-reward for data quality and runtime. A unified text processing methodology also reduced the runtime.\n",
    "\n",
    "Using the `scikit-learn` python module, the `TfidfVectorizer` submodule in the python submodule `feature_extraction.text` provides functionality for performing td-idf on a set of documents. The use of the built-in `scikit-learn` TF-IDF is based on the assumption that their TF-IDF calculator is far more sophisticated than any TF-IDF weight generator that can be manually coded. The purpose of generating this dataset is to observe the differences between a naive TF-IDF weighting and a far more complex iteration. The weighting and calculations are all left to the `scikit-learn` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the preprocessing and feature engineering steps automatically, executing a command directly from the notebook (with the warning that it takes a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a `dataset.csv` file, which we will be working for the rest of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !python preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train/test information from file reut2-000.sgm\n",
      "Extracting train/test information from file reut2-001.sgm\n",
      "Extracting train/test information from file reut2-002.sgm\n",
      "Extracting train/test information from file reut2-003.sgm\n",
      "Extracting train/test information from file reut2-004.sgm\n",
      "Extracting train/test information from file reut2-005.sgm\n",
      "Extracting train/test information from file reut2-006.sgm\n",
      "Extracting train/test information from file reut2-007.sgm\n",
      "Extracting train/test information from file reut2-008.sgm\n",
      "Extracting train/test information from file reut2-009.sgm\n",
      "Extracting train/test information from file reut2-010.sgm\n",
      "Extracting train/test information from file reut2-011.sgm\n",
      "Extracting train/test information from file reut2-012.sgm\n",
      "Extracting train/test information from file reut2-013.sgm\n",
      "Extracting train/test information from file reut2-014.sgm\n",
      "Extracting train/test information from file reut2-015.sgm\n",
      "Extracting train/test information from file reut2-016.sgm\n",
      "Extracting train/test information from file reut2-017.sgm\n",
      "Extracting train/test information from file reut2-018.sgm\n",
      "Extracting train/test information from file reut2-019.sgm\n",
      "Extracting train/test information from file reut2-020.sgm\n",
      "Extracting train/test information from file reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for file in os.listdir('data'):\n",
    "    data = open(os.path.join(os.getcwd(), \"data\", file), 'r')\n",
    "    text = data.read()\n",
    "    data.close()\n",
    "    tree = BeautifulSoup(text.lower(), \"html.parser\")\n",
    "    print(\"Extracting train/test information from file\", file)\n",
    "    for i in range(len(tree.find_all(\"reuters\"))):\n",
    "        if 'lewissplit=\"train\"' in str(tree.find_all(\"reuters\")[i]):\n",
    "            labels.append('train')\n",
    "        else:\n",
    "            labels.append('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Schibsted-Homework/dataset.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abel</th>\n",
       "      <th>abet</th>\n",
       "      <th>abid</th>\n",
       "      <th>...</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zirconium</th>\n",
       "      <th>zloti</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zweig</th>\n",
       "      <th>class-label:topics</th>\n",
       "      <th>Unnamed: 7836</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['cocoa']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['grain', 'wheat', 'corn', 'barley', 'oat', 's...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7837 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  aaron  abandon  abat  abbey  abbrevi  abduct  abel  abet  abid  ...  \\\n",
       "0   0    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0  ...   \n",
       "1   1    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0  ...   \n",
       "2   2    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0  ...   \n",
       "3   3    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0  ...   \n",
       "4   4    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   zimbabwean  zimmer  zinc  zirconium  zloti      zone  zurich  zweig  \\\n",
       "0         0.0     0.0   0.0        0.0    0.0  0.059531     0.0    0.0   \n",
       "1         0.0     0.0   0.0        0.0    0.0  0.000000     0.0    0.0   \n",
       "2         0.0     0.0   0.0        0.0    0.0  0.000000     0.0    0.0   \n",
       "3         0.0     0.0   0.0        0.0    0.0  0.000000     0.0    0.0   \n",
       "4         0.0     0.0   0.0        0.0    0.0  0.000000     0.0    0.0   \n",
       "\n",
       "                                  class-label:topics  Unnamed: 7836  \n",
       "0                                          ['cocoa']            NaN  \n",
       "1                                                 []            NaN  \n",
       "2                                                 []            NaN  \n",
       "3                                                 []            NaN  \n",
       "4  ['grain', 'wheat', 'corn', 'barley', 'oat', 's...            NaN  \n",
       "\n",
       "[5 rows x 7837 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace unlabeled documents with 'unlabeled'\n",
    "df['class-label:topics'] = df['class-label:topics'].apply(lambda x: \"['unlabeled']\" if x == '[]' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the first topic in cases where there were multiple topics\n",
    "topics = []\n",
    "for index, row in df.iterrows():\n",
    "    topics.append(ast.literal_eval(df['class-label:topics'][index])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new 'topics' column\n",
    "df['topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecesary columns\n",
    "del df['id']\n",
    "del df['class-label:topics']\n",
    "del df['Unnamed: 7836']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the labels column for the train/test split\n",
    "df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    14668\n",
       "test      6910\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cocoa', 'unlabeled', 'grain', 'veg-oil', 'earn', 'acq', 'wheat',\n",
       "       'copper', 'housing', 'money-supply', 'coffee', 'sugar', 'trade',\n",
       "       'reserves', 'ship', 'cotton', 'carcass', 'crude', 'nat-gas', 'cpi',\n",
       "       'money-fx', 'interest', 'gnp', 'soybean', 'meal-feed', 'alum',\n",
       "       'tea', 'oilseed', 'gold', 'tin', 'strategic-metal', 'livestock',\n",
       "       'retail', 'ipi', 'iron-steel', 'rubber', 'propane', 'heat', 'jobs',\n",
       "       'lei', 'bop', 'zinc', 'orange', 'pet-chem', 'dlr', 'gas', 'silver',\n",
       "       'wpi', 'fishmeal', 'hog', 'lumber', 'tapioca', 'instal-debt',\n",
       "       'lead', 'potato', 'l-cattle', 'rice', 'nickel', 'inventories',\n",
       "       'cpu', 'corn', 'fuel', 'jet', 'income', 'rand', 'platinum',\n",
       "       'saudriyal', 'nzdlr', 'palm-oil', 'coconut', 'rapeseed', 'stg',\n",
       "       'groundnut', 'wool', 'austdlr', 'soy-meal', 'plywood', 'barley',\n",
       "       'cruzado', 'yen', 'f-cattle', 'hk', 'naphtha'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topics'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['labels'] == 'train']\n",
    "y_train = df_train['topics']\n",
    "X_train = df_train.drop(['labels', 'topics'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['labels'] == 'test']\n",
    "y_test = df_test['topics']\n",
    "X_test = df_test.drop(['labels', 'topics'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abel</th>\n",
       "      <th>abet</th>\n",
       "      <th>abid</th>\n",
       "      <th>abidjan</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zirconium</th>\n",
       "      <th>zloti</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  abandon  abat  abbey  abbrevi  abduct  abel  abet  abid  abidjan  \\\n",
       "0    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0      0.0   \n",
       "1    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0      0.0   \n",
       "2    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0      0.0   \n",
       "3    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0      0.0   \n",
       "4    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0      0.0   \n",
       "\n",
       "   ...  ziegler  zimbabw  zimbabwean  zimmer  zinc  zirconium  zloti  \\\n",
       "0  ...      0.0      0.0         0.0     0.0   0.0        0.0    0.0   \n",
       "1  ...      0.0      0.0         0.0     0.0   0.0        0.0    0.0   \n",
       "2  ...      0.0      0.0         0.0     0.0   0.0        0.0    0.0   \n",
       "3  ...      0.0      0.0         0.0     0.0   0.0        0.0    0.0   \n",
       "4  ...      0.0      0.0         0.0     0.0   0.0        0.0    0.0   \n",
       "\n",
       "       zone  zurich  zweig  \n",
       "0  0.059531     0.0    0.0  \n",
       "1  0.000000     0.0    0.0  \n",
       "2  0.000000     0.0    0.0  \n",
       "3  0.000000     0.0    0.0  \n",
       "4  0.000000     0.0    0.0  \n",
       "\n",
       "[5 rows x 7834 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abel</th>\n",
       "      <th>abet</th>\n",
       "      <th>abid</th>\n",
       "      <th>abidjan</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zirconium</th>\n",
       "      <th>zloti</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zweig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aaron  abandon  abat  abbey  abbrevi  abduct  abel  abet  abid  \\\n",
       "12593    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0   \n",
       "12594    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0   \n",
       "12595    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0   \n",
       "12596    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0   \n",
       "12597    0.0      0.0   0.0    0.0      0.0     0.0   0.0   0.0   0.0   \n",
       "\n",
       "       abidjan  ...  ziegler  zimbabw  zimbabwean  zimmer  zinc  zirconium  \\\n",
       "12593      0.0  ...      0.0      0.0         0.0     0.0   0.0        0.0   \n",
       "12594      0.0  ...      0.0      0.0         0.0     0.0   0.0        0.0   \n",
       "12595      0.0  ...      0.0      0.0         0.0     0.0   0.0        0.0   \n",
       "12596      0.0  ...      0.0      0.0         0.0     0.0   0.0        0.0   \n",
       "12597      0.0  ...      0.0      0.0         0.0     0.0   0.0        0.0   \n",
       "\n",
       "       zloti  zone  zurich  zweig  \n",
       "12593    0.0   0.0     0.0    0.0  \n",
       "12594    0.0   0.0     0.0    0.0  \n",
       "12595    0.0   0.0     0.0    0.0  \n",
       "12596    0.0   0.0     0.0    0.0  \n",
       "12597    0.0   0.0     0.0    0.0  \n",
       "\n",
       "[5 rows x 7834 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest classifier and generate predictions\n",
    "rfc = RandomForestClassifier(random_state=176, n_estimators=1000).fit(X_train, y_train)\n",
    "rfc_preds = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, rfc_preds)\n",
    "balanced_acc = balanced_accuracy_score(y_test, rfc_preds)\n",
    "precision = precision_score(y_test, rfc_preds, average='macro')\n",
    "recall = recall_score(y_test, rfc_preds, average='macro')\n",
    "f1 = f1_score(y_test, rfc_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.719103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.184140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Precision</td>\n",
       "      <td>0.435045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Recall</td>\n",
       "      <td>0.181509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.224871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Metrics\n",
       "Accuracy           0.719103\n",
       "Balanced Accuracy  0.184140\n",
       "Precision          0.435045\n",
       "Recall             0.181509\n",
       "F1 Score           0.224871"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {'Metrics': [accuracy, balanced_acc, precision, recall, f1]}\n",
    "metrics_df = pd.DataFrame(metrics, index=['Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                 precision    recall  f1-score   support\\n\\n            acq       0.87      0.74      0.80       791\\n           alum       1.00      0.05      0.09        22\\n            bop       1.00      0.18      0.31        22\\n        carcass       0.00      0.00      0.00         9\\n          cocoa       1.00      0.26      0.42        19\\n        coconut       0.00      0.00      0.00         1\\n         coffee       1.00      0.59      0.74        27\\n         copper       1.00      0.20      0.33        25\\n           corn       0.00      0.00      0.00         1\\n         cotton       1.00      0.09      0.17        11\\n            cpi       0.64      0.35      0.45        26\\n            cpu       0.00      0.00      0.00         1\\n          crude       0.91      0.55      0.69       212\\n            dlr       0.00      0.00      0.00        26\\n           earn       0.67      0.92      0.78      1106\\n       f-cattle       0.00      0.00      0.00         2\\n           fuel       1.00      0.11      0.20         9\\n            gas       0.00      0.00      0.00        18\\n            gnp       1.00      0.04      0.08        47\\n           gold       1.00      0.08      0.15        36\\n          grain       0.78      0.76      0.77       165\\n      groundnut       0.00      0.00      0.00         2\\n           heat       0.83      0.62      0.71         8\\n             hk       0.00      0.00      0.00         1\\n            hog       0.50      0.75      0.60         8\\n        housing       1.00      0.33      0.50         3\\n         income       1.00      0.17      0.29         6\\n    instal-debt       0.00      0.00      0.00         2\\n       interest       0.93      0.34      0.50       120\\n            ipi       1.00      0.20      0.33        20\\n     iron-steel       0.00      0.00      0.00        21\\n            jet       0.00      0.00      0.00         2\\n           jobs       1.00      0.72      0.84        18\\n           lead       0.00      0.00      0.00        14\\n            lei       1.00      0.40      0.57         5\\n      livestock       1.00      0.06      0.12        16\\n         lumber       0.00      0.00      0.00         5\\n      meal-feed       0.00      0.00      0.00         7\\n       money-fx       0.70      0.42      0.53       206\\n   money-supply       0.83      0.55      0.66        44\\n        naphtha       0.00      0.00      0.00         1\\n        nat-gas       0.00      0.00      0.00        24\\n         nickel       0.00      0.00      0.00         2\\n        oilseed       1.00      0.29      0.45        31\\n         orange       1.00      0.22      0.36         9\\n       palm-oil       0.00      0.00      0.00         1\\n       pet-chem       0.00      0.00      0.00        11\\n       platinum       0.00      0.00      0.00         3\\n         potato       0.00      0.00      0.00         3\\n        propane       0.00      0.00      0.00         2\\n       rapeseed       0.00      0.00      0.00         0\\n       reserves       1.00      0.30      0.46        20\\n         retail       0.00      0.00      0.00         3\\n           rice       0.00      0.00      0.00         2\\n         rubber       0.00      0.00      0.00        10\\n           ship       0.70      0.27      0.39        71\\n         silver       0.00      0.00      0.00         9\\n        soybean       0.00      0.00      0.00         1\\n            stg       0.00      0.00      0.00         3\\nstrategic-metal       0.00      0.00      0.00        10\\n          sugar       0.97      0.60      0.74        48\\n            tea       0.00      0.00      0.00         5\\n            tin       0.00      0.00      0.00        14\\n          trade       0.71      0.46      0.56       154\\n      unlabeled       0.69      0.83      0.76      3323\\n        veg-oil       0.71      0.15      0.24        34\\n          wheat       0.00      0.00      0.00         3\\n            wpi       1.00      0.08      0.15        12\\n            yen       0.00      0.00      0.00         6\\n           zinc       0.00      0.00      0.00        11\\n\\n       accuracy                           0.72      6910\\n      macro avg       0.44      0.18      0.22      6910\\n   weighted avg       0.72      0.72      0.69      6910\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, rfc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
